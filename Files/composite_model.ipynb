{"cells":[{"cell_type":"markdown","source":["# Composite Model Deployment: Direct Lake + Import\n","\n","This notebook deploys a **Composite Semantic Model** based on **Direct Lake** and **Import** storage modes. Import tables can come from any supported data source and relationships between Direct Lake on OneLake and Import tables are **regular relationships**. Small dimension or lookup tables already in Direct Lake storage mode can instead use import storage mode, giving you the option to extend the table with **calculated columns** and structuring the table with **hierarchies** for use in Power BI reports and Excel pivot tables. The feature was announced in [Power BI May 2025](https://powerbi.microsoft.com/en-us/blog/power-bi-may-2025-feature-summary/#post-29934-_Toc119482792) release.\n","\n","Requirements:\n","- **Direct Lake on One Lake semantic model:** This model is based on the _AzureStorage.DataLake_ protocol. The model must be created in the **Power BI Desktop**, otherwise it will use the _Sql.Database_ protocol.\n","- **Import semantic model:** This model will be used as a reference for replacement and deployment. Only the desired tables to be in Import mode are required.\n","- **XMLA read/write:** This will enable the deployment through XMLA endpoint. The permission is enabled at the **Capacity level** and permissions are managed at the **Admin Portal**.\n","   \n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6193528a-474d-47e2-9481-4de11175b5da"},{"cell_type":"markdown","source":["### Libraries\n","The only **mandatory** library is **Semantic Link Labs**, a Python library designed for use in **Microsoft Fabric notebooks**. This library extends the capabilities of **Semantic Link** offering additional functionalities to seamlessly integrate and work alongside it. In this notebook, Semantic Link Labs is used to capture the reference models definition files and to create/update the Composite Model.\n","\n","Remaining libraries are **optional**, and are used to export the Composite Model definition file to a Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"99f45258-7535-4e51-b240-f2fdc27aef13"},{"cell_type":"code","source":["# Install Semantic Link Labs\n","%pip install semantic-link-labs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"7c92bdfc-caa7-4b81-adc5-be0fe1015958"},{"cell_type":"code","source":["# Required Library\n","import sempy_labs as labs\n","\n","# Optional Libraries\n","import json\n","from notebookutils import fs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"37274616-5892-4a43-992d-8ea00b8c1887"},{"cell_type":"markdown","source":["### Parameters\n","\n","All the parameters expect the **name** or the **id** of the object, except the **import_tables** parameter. This one requires a list of names of the tables that will be in Import mode."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"824933e4-d9ec-4686-866f-037170eefcc8"},{"cell_type":"code","source":["# Required parameters\n","workspace = 'ws_demo'                           # Reference workspace\n","\n","dataset_import = 'sm_import'                    # Source: Import model\n","dataset_directlake ='sm_directlake'             # Source: Direct Lake model\n","dataset_composite = 'sm_composite'              # Sink: Composite model\n","\n","import_tables = ['table1', 'table2', 'table3']  # List of tables that will be set to Import mode\n","\n","# Optional parameters\n","storage = 'lh_demo'                             # Lakehouse to ouput the Composite Model definition file"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":[]},"id":"ca4b6e9b-6866-4727-b893-98c44fe18931"},{"cell_type":"markdown","source":["### Model Definition\n","\n","Process workflow:\n","1. Capture the original model definition (BIM) files\n","2. Remove from the Direct Lake file the tables that will be kept in Import mode\n","3. Add in the Direct Lake file the Import tables based on the Import file definition\n","4. Deploy the model\n","\n","Existing relationships on the Direct Lake model are **persisted**. It will fail if the column names are different (between Direct Lake and Import models)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f8cb4fd3-dc99-4fdf-85ef-432338a3e2a8"},{"cell_type":"code","source":["# Get Model Definition file (BIM) from source models\n","bim_import = labs.get_semantic_model_bim( dataset = dataset_import, workspace = workspace )\n","bim_directlake = labs.get_semantic_model_bim( dataset = dataset_directlake, workspace = workspace )\n","\n","# Delete the tables that will change storage mode from the Direct Lake file\n","bim_directlake['model']['tables'] = [\n","    table for table in bim_directlake['model']['tables'] \n","    if table['name'] not in import_tables\n","]\n","\n","# Include the tables with changed storage mode to Import in the Direct Lake file\n","for table in bim_import['model']['tables']:\n","    if table['name'] in import_tables:\n","        bim_directlake['model']['tables'].append(table)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e1c7a5f-8e3d-4fc6-bf4b-14dfc0d9fcb8"},{"cell_type":"markdown","source":["### Deployment\n","\n","The deployment is done via **XMLA endpoint**. It will create or update the model if it already exists. After the deployment, a new connection will be displayed in the **Gateway and Cloud Connections** section of the Semantic Model Properties. A **rebind** of the Import tables connection is required, since it cannot rely on **Single Sign On** as Direct Lake does (this is only required after the first deployment).\n","\n"," - If a connection already exists for the Import table, just check it on the Map to.\n"," - Otherwise, create a new connection and then Map to it.\n","\n"," After the rebind the semantic model can be **refreshed**."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5fde1b24-669d-4347-af54-7484d1d36cf5"},{"cell_type":"code","source":["# Create/Update the Composite Model\n","try:\n","    labs.create_semantic_model_from_bim( dataset = dataset_composite, bim_file = bim_directlake, workspace = workspace )\n","except:\n","    labs.update_semantic_model_from_bim( dataset = dataset_composite, bim_file = bim_directlake, workspace = workspace )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dbb2f867-2805-49bc-96ae-a6243e338e02"},{"cell_type":"markdown","source":["## Optional Steps\n","\n","The steps below are **optional**. \n","- The first one validates if the Import connection has been **binded** (this step must be done manually after deployment).\n","- The last one outptus the Composite Model definition file to a **Lakehouse**."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a3ecdf45-6db2-48f3-81fa-0cf4cc0f954d"},{"cell_type":"markdown","source":["### Analyze Connection Bindings"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"65a7aa3f-c70e-4e7a-bd3b-9d9d19f412f3"},{"cell_type":"code","source":["# Retrieves the list of connections visible in the Fabric environment\n","connections = labs.list_connections()\n","\n","# Retrieves the list of connection dependencies in the Semantic Model\n","model_connections = labs.list_item_connections( item_name = dataset_composite, item_type = 'SemanticModel', workspace = workspace )\n","\n","# Connections that do not rely on AzureDataLakeStorage (Direct Lake on Onelake)\n","connection_ids = model_connections[ model_connections['Connection Type'] != 'AzureDataLakeStorage' ]['Connection Id']\n","\n","for con in connection_ids:\n","    connection_path = model_connections[ model_connections['Connection Id'] == con ]['Connection Path'].iloc[0]\n","\n","    if con:\n","        connection_name = connections[ connections['Connection Id'] == con ]['Connection Name'].iloc[0]\n","        print(f'The path {connection_path} has been mapped to the {connection_name} connection.')\n","\n","    else:\n","        print(f'No connection found for path {connection_path}. Bind the connection before refreshing the semantic model.')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f4465521-06a9-48cd-b7ba-a82906e6acdb"},{"cell_type":"markdown","source":["### Output Model Definition File"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"80c9ec44-6f6f-49cb-b5c7-21bac289b493"},{"cell_type":"code","source":["# Save model definition to Lakehouse\n","path = f'abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{storage}.Lakehouse/Files/{dataset_composite}.json'\n","file = json.dumps( bim_directlake, indent = 2 )\n","\n","fs.put( path, file, overwrite = True )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9be7c6c6-20c9-4044-ab76-ba5dc206bbe2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}